{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64e6a19c",
   "metadata": {},
   "source": [
    "# **Part 2: DATA PROCESSING**\n",
    "**Objective:** Clean, transform, and prepare data for the model using **NumPy only**.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. Load Data & Handle Missing Values.\n",
    "2. **Feature Engineering:** Log-transform 'Amount' and extract 'Hour' from 'Time'.\n",
    "3. **Stratified Split:** Split data into Train/Val/Test while preserving the Fraud ratio.\n",
    "4. **Normalization:** Apply Z-score scaling (Fit on Train -> Transform Val/Test) to prevent data leakage.\n",
    "5. Save processed data for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2627a",
   "metadata": {},
   "source": [
    "## **Setup & Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eb000e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from src.data_processing import (\n",
    "    load_csv_numpy, \n",
    "    impute_missing, \n",
    "    extract_time_features, \n",
    "    stratified_split, \n",
    "    save_processed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a913480",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Load and Clean Data**\n",
    "We load the raw CSV and check for missing values. If any exist, we fill them with the column median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54501f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X shape: (284807, 30)\n",
      "Original y shape: (284807,)\n",
      "Missing values handled.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "header, data = load_csv_numpy('../data/raw/creditcard.csv')\n",
    "\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "print(f\"Original X shape: {X.shape}\")\n",
    "print(f\"Original y shape: {y.shape}\")\n",
    "\n",
    "# Handle Missing Values\n",
    "# Although this dataset is usually clean, we apply this for robustness\n",
    "X_clean = impute_missing(X, strategy='median')\n",
    "print(\"Missing values handled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f86bb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Feature Engineering**\n",
    "Based on our EDA (Notebook `01_data_exploration.ipynb`), we know:\n",
    "- **Amount:** Highly skewed. We apply `Log(1+x)` to make it more normal.\n",
    "- **Time:** The raw seconds are not useful. We convert them to \"Hour of Day\" (0-23) to capture the 2 AM fraud pattern.\n",
    "- **V1-V28:** Already PCA-transformed, so we keep them as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a220b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after Feature Engineering: (284807, 30)\n",
      "New Feature Order: V1-V28 (0-27), LogAmount (28), Hour (29)\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract V1-V28 (Indices 1 to 28)\n",
    "v_features = X_clean[:, 1:29]\n",
    "\n",
    "# 2. Transform Amount (Index 29)\n",
    "# Use log1p to avoid log(0) errors\n",
    "amount_col = X_clean[:, 29]\n",
    "amount_log = np.log1p(amount_col).reshape(-1, 1)\n",
    "\n",
    "# 3. Transform Time (Index 0)\n",
    "time_col = X_clean[:, 0]\n",
    "hour_feature = extract_time_features(time_col)\n",
    "\n",
    "# 4. Concatenate Features\n",
    "# New structure: [V1...V28, LogAmount, Hour]\n",
    "X_engineered = np.hstack([v_features, amount_log, hour_feature])\n",
    "\n",
    "print(f\"Shape after Feature Engineering: {X_engineered.shape}\")\n",
    "print(\"New Feature Order: V1-V28 (0-27), LogAmount (28), Hour (29)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c07ac",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Stratified Train-Val-Test Split**\n",
    "Because fraud cases are rare (0.17%), random splitting might put all frauds in the Test set. \n",
    "\n",
    "We use **Stratified Splitting** to ensure the fraud ratio is consistent across all sets.\n",
    "- **Train:** 70%\n",
    "- **Validation:** 15%\n",
    "- **Test:** 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89150937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (199364, 30) | Fraud Count: 344\n",
      "Val Shape:   (42720, 30) | Fraud Count: 73\n",
      "Test Shape:  (42723, 30) | Fraud Count: 75\n"
     ]
    }
   ],
   "source": [
    "# Note: We split BEFORE Normalization to avoid Data Leakage\n",
    "(X_train_raw, y_train), (X_val_raw, y_val), (X_test_raw, y_test) = stratified_split(\n",
    "    X_engineered, y, \n",
    "    train_frac=0.7, val_frac=0.15, test_frac=0.15, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Train Shape: {X_train_raw.shape} | Fraud Count: {np.sum(y_train==1)}\")\n",
    "print(f\"Val Shape:   {X_val_raw.shape} | Fraud Count: {np.sum(y_val==1)}\")\n",
    "print(f\"Test Shape:  {X_test_raw.shape} | Fraud Count: {np.sum(y_test==1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df80c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Normalization (Standardization)**\n",
    "We apply **Z-score Standardization**: $z = \\frac{x - \\mu}{\\sigma}$.\n",
    "\n",
    "**Crucial Rule:** We calculate $\\mu$ (mean) and $\\sigma$ (std) using **ONLY the Training Set**. \n",
    "\n",
    "Then we use those values to transform the Validation and Test sets. This simulates a real-world scenario where we don't know the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b9f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete.\n",
      "Train Mean (approx 0): -0.0000\n",
      "Train Std  (approx 1): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 1. Calculate Mean and Std from TRAIN set\n",
    "mean_train = X_train_raw.mean(axis=0)\n",
    "std_train = X_train_raw.std(axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "std_train[std_train == 0] = 1e-8\n",
    "\n",
    "# 2. Apply to Train\n",
    "X_train = (X_train_raw - mean_train) / std_train\n",
    "\n",
    "# 3. Apply to Val and Test using TRAIN statistics\n",
    "X_val = (X_val_raw - mean_train) / std_train\n",
    "X_test = (X_test_raw - mean_train) / std_train\n",
    "\n",
    "print(\"Normalization complete.\")\n",
    "print(f\"Train Mean (approx 0): {np.mean(X_train):.4f}\")\n",
    "print(f\"Train Std  (approx 1): {np.std(X_train):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899dd0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for NaNs in final arrays...\n",
      "Data is clean.\n"
     ]
    }
   ],
   "source": [
    "# Final Check before saving\n",
    "print(\"Checking for NaNs in final arrays...\")\n",
    "if np.isnan(X_train).any() or np.isnan(X_val).any() or np.isnan(X_test).any():\n",
    "    print(\"WARNING: NaNs detected!\")\n",
    "else:\n",
    "    print(\"Data is clean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd0061",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Save Processed Data**\n",
    "Save everything into a compressed `.npz` file for the Modeling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c7d8255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved successfully at: ../data/processed/processed.npz\n"
     ]
    }
   ],
   "source": [
    "savepath = save_processed('../data/processed', \n",
    "                          X_train=X_train, y_train=y_train,\n",
    "                          X_val=X_val, y_val=y_val,\n",
    "                          X_test=X_test, y_test=y_test)\n",
    "\n",
    "print(f\"Processed data saved successfully at: {savepath}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
